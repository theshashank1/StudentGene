import os
import sys
import traceback
import json

# Add the project root directory to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", "..", ".."))
print(project_root)
sys.path.insert(0, project_root)

from api.services.model import ModelService
from api.utils.load_pdf import load_pdf

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

class ChatService:
    def __init__(self, pdf_path):
        self.service = ModelService()
        self.llm = self.service.get_llm_model()

        if self.llm is None:
            raise Exception('LLM not found')

        try:
            doc = load_pdf(pdf_path)

            # This will split the document into chunks with a specified size and overlap
            split = RecursiveCharacterTextSplitter(
                chunk_size=500,
                chunk_overlap=50,
                length_function=len
            )

            # Splitting the document into chunks for better processing
            split_docs = split.split_documents(doc)

            print(f"Number of document chunks: {len(split_docs)}")

            self.embeddings = self.service.get_embedding_model()

            # Creating a Chroma vector database from the document chunks and embeddings
            self.vectorstore = Chroma.from_documents(
                documents=split_docs,  # Data: the split document chunks
                embedding=self.embeddings,  # Embedding model
                persist_directory="./chroma_db"  # Directory to save the vector database
            )

            self.retriever = self.vectorstore.as_retriever()


            # Define a template for the prompt
            template = """
            Answer the question based only on the following context:

            {context}

            Question: {question}
            """

            # Create a prompt template from the defined template
            self.prompt = ChatPromptTemplate.from_template(template)

            # Define the chain or pipeline to execute the retrieval, prompt, model, and output parsing
            self.chain = (
                {"context": self.retriever, "question": RunnablePassthrough()}
                | self.prompt
                | self.llm
                | StrOutputParser()
            )

        except Exception as e:
            print(f"An error occurred during initialization: {str(e)}")
            traceback.print_exc()
            raise

    def chat(self, question):
        try:
            # Invoke the chain with the question
            output = self.chain.invoke(question)
            json_data = json.loads(output)
            return json_data
        except Exception as e:
            print(f"An error occurred during chat: {str(e)}")
            traceback.print_exc()
            return "I'm sorry, but I encountered an error while processing your question."

if __name__ == "__main__":
    try:
        # pdf_path = r"C:\Users\hp\Downloads\pdf-sample.pdf"  # Replace with the correct path to your PDF
        pdf_path="..\media\store\pdfs\pdf-sample.pdf"
        chat_service = ChatService(pdf_path)

        while True:
            question = input("Enter your question (or 'quit' to exit): ")
            if question.lower() == 'quit':
                break
            response = chat_service.chat(question)
            print("Response:", response)
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        traceback.print_exc()